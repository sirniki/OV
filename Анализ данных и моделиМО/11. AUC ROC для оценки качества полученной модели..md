**AUC-ROC** (Area Under the Receiver Operating Characteristic Curve) — это популярная метрика для оценки качества бинарных классификаторов. ROC-кривая представляет собой график, показывающий соотношение между True Positive Rate (TPR) и False Positive Rate (FPR) при различных порогах классификации.

### Определения

- **True Positive Rate (TPR) или Recall (Полнота)**: Доля правильно предсказанных положительных наблюдений среди всех реальных положительных наблюдений.
	 $TPR=\frac{TP}{TP+FN}$
- **False Positive Rate (FPR)**: Доля неправильно предсказанных положительных наблюдений среди всех реальных отрицательных наблюдений.
	$FPR=\frac{FP}{FP+TN}$​

### ROC-кривая
ROC-кривая строится по следующим шагам:

1. **Вычисление TPR и FPR**: Для каждого порога классификации вычисляются значения TPR и FPR.
2. **Построение графика**: На оси xxx откладывается FPR, а на оси y — TPR.
### AUC (Area Under the Curve)

AUC (площадь под кривой) представляет собой численное значение, которое измеряет качество модели. Значение AUC варьируется от 0 до 1:

- **AUC = 1**: Идеальный классификатор.
- **AUC = 0.5**: Классификатор, который работает на уровне случайного угадывания.
- **AUC < 0.5**: Классификатор, который работает хуже случайного угадывания.

### Преимущества AUC-ROC

1. **Независимость от порога**: AUC-ROC учитывает все возможные пороги классификации, что делает метрику более информативной по сравнению с фиксированным порогом.
2. **Интерпретируемость**: AUC-ROC дает интуитивное понимание качества модели. Чем ближе AUC к 1, тем лучше модель.
3. **Сравнение моделей**: AUC-ROC позволяет сравнивать качество разных моделей независимо от выбранного порога.

### Недостатки AUC-ROC

1. **Несбалансированные данные**: AUC-ROC может давать оптимистичные оценки для моделей, обученных на несбалансированных данных.
2. **Чувствительность к редким событиям**: AUC-ROC может быть менее информативной для моделей, работающих с редкими событиями (например, обнаружение редких заболеваний).

### Пример использования AUC-ROC в Python

Пример вычисления AUC-ROC с использованием библиотеки `scikit-learn`:

python

Копировать код

`from sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt  # Предполагаем, что у нас есть истинные метки y_test и предсказанные вероятности y_pred_prob  # Вычисление ROC-кривой fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)  # Вычисление AUC roc_auc = auc(fpr, tpr)  # Построение графика ROC-кривой plt.figure() plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc) plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver Operating Characteristic') plt.legend(loc="lower right") plt.show()`

### Заключение

AUC-ROC — мощная метрика для оценки бинарных классификаторов, предоставляющая обширное понимание качества модели независимо от порога классификации. Однако важно учитывать контекст использования и особенности данных, чтобы правильно интерпретировать результаты.