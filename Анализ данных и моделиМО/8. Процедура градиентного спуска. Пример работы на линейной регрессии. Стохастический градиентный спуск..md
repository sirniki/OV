Градиентный спуск — это фундаментальный алгоритм оптимизации, широко используемый в машинном обучении и глубоком обучении.
### Процедура градиентного спуска

1. **Инициализация параметров:**
    
    - Задаются начальные значения для параметров модели.
2. **Выбор функции стоимости:**
    
    - Определяется функция, которую необходимо минимизировать, например, среднеквадратичная ошибка (MSE).
3. **Вычисление градиента:**
    - Находится вектор частных производных функции стоимости по параметрам модели.
4. **Обновление параметров:**
    
    - Параметры обновляются в направлении, противоположном градиенту, с использованием шага обучения ($α$): 
		    $θ=θ−α∇J(θ)$
1. **Проверка критерия сходимости:**
    
    - Процесс повторяется до тех пор, пока не достигнуто условие остановки, например, малая величина изменения функции стоимости или достижение максимального числа итераций.

### Применение

- **Линейная регрессия:** Оптимизация коэффициентов модели для минимизации ошибки.
- **Нейронные сети:** Обучение весов сети для повышения точности предсказаний.

### Преимущества

- **Простота:** Легко реализуется и понятен.
- **Гибкость:** Применим к различным типам моделей.

### Недостатки

- **Может застрять в локальных минимумах.**
- **Чувствителен к выбору шага обучения.**
### Пример работы градиентного спуска

**Задача:** Предсказание значения yyy на основе одного признака xxx.

1. **Инициализация параметров:**
    
    - Начальные значения коэффициентов $θ_0$​ и $θ_1$ задаются случайно.
2. **Функция стоимости:**
    
    - Используется среднеквадратичная ошибка (MSE): $J(θ_0,θ_1)=\frac{1}{2m}\sum\limits_{i=1}^{m} (h(x_i)−y_i)^2$
    - Где $h(x_i)=θ_0+θ_1x_i$ — предсказанное значение.
3. **Вычисление градиента:**
    - Градиенты по $θ_0$ и $θ_1$​: 
    - 
		- $\frac{∂J}{∂θ_0}=\frac{1}{m}\sum\limits_{i=1}^{m} (h(x_i)−y_i)$
		- $\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h(x_i) - y_i)x_i$
1. **Обновление параметров:**
    
    - Параметры обновляются с использованием шага обучения (α): 
		- $θ_0=θ_0−α\frac{∂J}{∂θ_0}$
		- $θ_1​=θ_1​−α\frac{​∂J}{∂θ_1}​$
1. **Повторение итераций:**
    
    - Процесс повторяется до сходимости, когда изменения функции стоимости становятся незначительными.

Стохастический градиентный спуск (Stochastic Gradient Descent, SGD) — это вариант классического градиентного спуска, который используется для оптимизации функций стоимости, особенно в задачах машинного обучения, таких как обучение моделей регрессии и классификации на больших данных. В отличие от обычного градиентного спуска, который вычисляет градиент по всем обучающим примерам, стохастический градиентный спуск обновляет параметры модели по одному случайно выбранному обучающему примеру за одну итерацию, что делает его быстрее и более эффективным на больших объемах данных.

### Процесс стохастического градиентного спуска

1. **Инициализация параметров:**
    
    - Начальные значения параметров модели (например, весов) задаются случайным образом.
2. **Выбор случайного обучающего примера:**
    
    - На каждой итерации выбирается случайный обучающий пример из обучающего набора данных.
3. **Вычисление градиента на одном примере:**
    
    - Для выбранного примера вычисляется градиент функции стоимости по параметрам модели.
4. **Обновление параметров:**
    
    - Параметры модели обновляются в направлении, противоположном градиенту, с использованием шага обучения (α): $θ=θ−α∇J(θ;x_i,y_i)$
    - Где θ\thetaθ — параметры модели, $∇J(θ;x_i,y_i)$ — градиент функции стоимости по параметрам для выбранного обучающего примера $(x_i,y_i)$.
5. **Повторение итераций:**
    
    - Процесс повторяется до достижения критерия остановки, например, заданного числа итераций или достижения минимального изменения функции стоимости.

### Преимущества стохастического градиентного спуска

- **Эффективность на больших данных:** Позволяет обучать модели на больших объемах данных из-за меньшего времени вычислений на каждой итерации.
- **Лучшая обобщающая способность:** Использование случайных примеров помогает избежать застревания в локальных минимумах и может улучшить обобщающую способность модели.

### Недостатки стохастического градиентного спуска

- **Неустойчивость:** Из-за случайного выбора примеров на каждой итерации SGD может быть менее стабильным по сравнению с обычным градиентным спуском.
- **Необходимость тонкой настройки параметров:** Требуется настройка параметров, таких как размер шага (α\alphaα), чтобы достичь оптимальной сходимости.

### Применение стохастического градиентного спуска

SGD широко применяется в обучении моделей машинного обучения, таких как линейная регрессия, логистическая регрессия, нейронные сети и другие, особенно когда требуется обработка больших объемов данных или когда данные поступают в потоковом режиме.

Этот метод является фундаментальным инструментом в арсенале методов оптимизации и играет ключевую роль в разработке и обучении современных моделей машинного обучения.