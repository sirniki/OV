Логистическая регрессия — это метод статистического анализа, используемый для прогнозирования вероятности наступления одного из двух возможных исходов (дихотомическая зависимая переменная). Этот метод относится к классу линейных моделей и часто используется в задачах классификации.

### Процедура обучения

Процесс обучения модели логистической регрессии включает следующие основные шаги:

1. **Определение модели**: Модель логистической регрессии использует логистическую функцию (сигмоидальную функцию) для преобразования линейной комбинации входных переменных в вероятности.
	$P(y=1∣X)=σ(β_0​+β_1​X_1​+β_2​X_2​+...+β_n​X_n​)=\frac{1}{1+e^{−(β_0​+β_1​X_1​+β2​X_2​+...+β_n​X_n​)}}$
2. **Функция потерь**: Для логистической регрессии используется логарифмическая функция потерь (log-loss), которая измеряет расхождение между предсказанными вероятностями и фактическими метками классов.
	$log-loss=−\frac{1}{N}​\sum\limits_{i=1}^{N}​[y_i​log(ŷ_i​)+(1−y_i​)log(1−ŷ_i​)]$
3. **Оптимизация**: Для минимизации функции потерь и нахождения оптимальных параметров модели (β\betaβ) используется метод максимального правдоподобия. Чаще всего применяется метод градиентного спуска или его модификации.
		$β:=β−α\frac{∂log-loss​}{∂β}$
4. **Оценка модели**: После обучения модель оценивается по различным метрикам (точность, полнота, F1-мера, ROC-AUC), чтобы определить её качество на тестовых данных.
### Связь с линейной регрессией

Логистическая регрессия тесно связана с линейной регрессией:

1. **Линейная комбинация признаков**: Как и в линейной регрессии, в логистической регрессии используется линейная комбинация входных признаков $(β_0+β_1X_1+β_2X_2+...+β_nX_n)$
2. **Различие в зависимой переменной**: В линейной регрессии зависимая переменная является непрерывной, тогда как в логистической регрессии она дискретная (бинарная).
3. **Функция активации**: В логистической регрессии используется логистическая функция для преобразования линейной комбинации в вероятность. В линейной регрессии выход модели является просто линейной комбинацией входных данных без применения функции активации.
4. **Функция потерь**: В логистической регрессии используется логарифмическая функция потерь, в то время как в линейной регрессии обычно используется среднеквадратичная ошибка (MSE).
5. **Цель модели**: Линейная регрессия предназначена для регрессии, то есть предсказания непрерывных значений, тогда как логистическая регрессия предназначена для классификации, то есть предсказания вероятности принадлежности к одному из двух классов.

## Проблема Underflow для `log(likelihood)`

Underflow (понижение точности до нуля) — это проблема, возникающая при вычислениях с числами, которые слишком малы, чтобы их можно было точно представить в компьютере. В контексте логистической регрессии, где используются вероятности, которые могут быть очень близки к нулю, проблема underflow может возникнуть при вычислении логарифма функции правдоподобия (log-likelihood).

### Причины underflow

1. **Малые вероятности**: Вероятности, предсказываемые логистической регрессией, могут быть очень близки к нулю (например, 10−1010^{-10}10−10). При взятии логарифма таких малых чисел возникает underflow, так как логарифм малых чисел дает большие отрицательные значения.
    
2. **Множество наблюдений**: Если у вас много наблюдений в данных, малые вероятности могут умножаться, что приводит к еще меньшим значениям.
    

### Формулировка функции правдоподобия

Функция правдоподобия в логистической регрессии выглядит следующим образом:
$L(β)=\prod\limits_{i=1}^{N}​P(y_i​∣X_i​;β)^{y_i}​⋅(1−P(y_i​∣X_i​;β))^{1−y_i}​$
Где:

- $P(y_i​∣X_i​;β)$ — вероятность класса 1 для наблюдения i,
- $y_i$ — истинная метка класса для наблюдения i.
### Логарифм функции правдоподобия

Для предотвращения underflow, используют логарифм функции правдоподобия, что позволяет превратить произведение в сумму:
$logL(β)=x\sum\limits_{i=1}^{N}​[y_i​log(P(y_i​∣X_i​;β))+(1−y_i​)log(1−P(y_i​∣X_i​;β))]$
### Проблема underflow

Если $P(y_i​∣X_i​;β)$  или $1  - P(y_i|X_i;β)$ очень малы, $log⁡(P(y_i​∣X_i​;β))$ или $log(1  - P(y_i|X_i;β))$ могут быть очень большими отрицательными числами, что приводит к проблемам точности в вычислениях.
### Решения проблемы underflow

1. **Использование лог-суммы-экспоненты (log-sum-exp)**: Это стабильный способ вычисления логарифма суммы экспонент. Он позволяет избежать underflow, проводя вычисления в логарифмическом пространстве.
    
    Формула для log-sum-exp:
    $log(\sum\limits_{i=1}^N​e^{x_i​})=max(x_i​)+log(\sum\limits_{i=1}^N​e^{x_i​-max(x_i)})$
    Это помогает избежать слишком маленьких значений экспонент.
    
2. **Работа в логарифмическом пространстве**: Вместо работы с вероятностями, можно работать с их логарифмами. Это позволяет избежать проблем с малыми числами.
    
3. **Регуляризация**: Добавление регуляризации (например, L2-регуляризация) к функции потерь может помочь стабилизировать обучение и избежать слишком малых вероятностей.
    
4. **Проверка чисел на малость**: Включение проверки и обработки чисел, которые слишком малы (например, заменять значения меньше определенного порога на этот порог), может помочь избежать underflow.
### Пример реализации

Вот пример на Python, показывающий использование log-sum-exp для стабилизации вычислений:

python

`import numpy as np  def log_sum_exp(x):     max_x = np.max(x)     return max_x + np.log(np.sum(np.exp(x - max_x)))  # Пример вычисления log-sum-exp для массива x = np.array([-1000, -1001, -1002]) lse = log_sum_exp(x) print(lse)`

### Заключение

Проблема underflow при вычислении логарифма функции правдоподобия в логистической регрессии может быть серьезной, но существует несколько методов для её решения. Использование логарифмических преобразований и методов, таких как log-sum-exp, позволяет значительно уменьшить риск потери точности при работе с малыми вероятностями.